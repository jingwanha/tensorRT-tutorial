{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from utils import *\n",
    "from generator_v1 import *\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Configuration Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1 BASE CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASE CONFIGURATION\n",
    "\n",
    "class _config(Config):\n",
    "    PROJ_NAME = 'diebonder'\n",
    "    MODE=[\"train\",\"eval\"][0]\n",
    "    MODEL_NAME=[\"InceptionResNetV2\", \"DenseNet201\", \"NASNetMobile\", \"NASNetLarge\", \"EfficientNet\"][3]\n",
    "    \n",
    "    DATA_GLOB='/raid/nvidia/jupyterhub/notebook/int/jingwan_diebonder/Data/PoC/chest_xray/*/*/*.*'\n",
    "    SAVE_BASE_DIR='./logs'\n",
    "    \n",
    "    LABEL_MAP=OrderedDict({\"normal\":0, \"pneumonia\":1})\n",
    "    \n",
    "    GPU_MIN_MEM=30000\n",
    "    GPU_OPTION=\"auto\"\n",
    "    \n",
    "    CLASS_WEIGHTS=False\n",
    "    BATCH_SIZE=16\n",
    "    NUM_EPOCH=200\n",
    "    LR=1e-06\n",
    "    \n",
    "model_config=_config()\n",
    "model_config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2 DATA CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA RELATED CONFIGURATION\n",
    "\n",
    "# image laod\n",
    "img_list = glob(model_config.DATA_GLOB)\n",
    "\n",
    "data_df=pd.DataFrame({\"image\": img_list,\n",
    "                      \"label_name\": list(map(lambda x : x.split(\"/\")[-2], img_list)),\n",
    "                      \"label\": list(map(lambda x : model_config.LABEL_MAP[x.split(\"/\")[-2]], img_list)),\n",
    "                      \"train_test\": list(map(lambda x : x.split(\"/\")[-3], img_list))\n",
    "                      })\n",
    "\n",
    "# print data info\n",
    "print (data_df.groupby(['train_test','label_name'])['image'].count())\n",
    "\n",
    "# train / test dataset split\n",
    "train_df = data_df.loc[data_df['train_test']=='train'].drop(['train_test'],axis=1).reset_index(drop=True)\n",
    "test_df = data_df.loc[data_df['train_test']=='test'].drop(['train_test'],axis=1).reset_index(drop=True)\n",
    "\n",
    "# apply class weight\n",
    "y_label=train_df[\"label_name\"]\n",
    "if model_config.CLASS_WEIGHTS :\n",
    "    class_weights=compute_class_weight('balanced', np.unique(y_label), y_label)\n",
    "else :\n",
    "    class_weights=None\n",
    "    \n",
    "print ('class weight: {} = {}'.format(np.unique(y_label),class_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3 GPU CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU CONFIGURATION\n",
    "while True : \n",
    "    \n",
    "    # auto 인 경우 아무 조건에 맞는 할당 가능한 gpu 번호 확인 \n",
    "    if model_config.GPU_OPTION == \"auto\" :\n",
    "        gpu_available=ckech_available_gpu(num_gpu=model_config.GPU_NUM, min_mem_mb=model_config.GPU_MIN_MEM)\n",
    "        \n",
    "        if len(gpu_available) > 0 : model_config.GPU_OPTION=gpu_available\n",
    "        else : model_config.GPU_OPTION=\"99\"\n",
    "            \n",
    "    # 조건에 맞는 할당 가능 gpu 번호 확인\n",
    "    if isinstance(model_config.GPU_OPTION, str) : _set_gpu=model_config.GPU_OPTION\n",
    "    elif isinstance(model_config.GPU_OPTION, (list, tuple, np.ndarray)): _set_gpu=\",\".join(np.array(model_config.GPU_OPTION, np.str))\n",
    "    else : _set_gpu=\"99\"\n",
    "        \n",
    "    if model_config.ALLOW_CPU :break\n",
    "        \n",
    "    # gpu 할당 대기\n",
    "    elif _set_gpu == \"99\" :\n",
    "        time.sleep(5*60)\n",
    "        print(\"Wait for GPU.\", end='\\r')\n",
    "        \n",
    "    else: break\n",
    "\n",
    "# gpu 할당\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=_set_gpu\n",
    "\n",
    "if _set_gpu == \"99\": print(\"CPU is set.\")\n",
    "else : print(\"GPU is set '{}'.\".format(_set_gpu))\n",
    "\n",
    "# Set session\n",
    "tf_config = tf.ConfigProto()\n",
    "tf_config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config = tf_config))\n",
    "\n",
    "from setproctitle import setproctitle\n",
    "setproctitle(\"{}_{}_{}\".format(model_config.PROJ_NAME,model_config.MODE,model_config.MODEL_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-4 MODEL CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL CONFIGURATION\n",
    "\n",
    "# Set Model\n",
    "if model_config.MODEL_NAME==\"InceptionResNetV2\":\n",
    "    from keras.applications.inception_resnet_v2 import InceptionResNetV2 as base_model\n",
    "    model_config.INPUT_SHAPE=(299,299,3)\n",
    "\n",
    "elif model_config.MODEL_NAME==\"DenseNet201\":\n",
    "    from keras.applications.densenet import DenseNet201 as base_model\n",
    "    model_config.INPUT_SHAPE=(224,224,3)\n",
    "\n",
    "elif model_config.MODEL_NAME==\"NASNetMobile\":\n",
    "    from keras.applications.nasnet import NASNetMobile as base_model\n",
    "    model_config.INPUT_SHAPE=(224,224,3)\n",
    "\n",
    "elif model_config.MODEL_NAME==\"NASNetLarge\":\n",
    "    from keras.applications.nasnet import NASNetLarge as base_model\n",
    "    model_config.INPUT_SHAPE=(331,331,3)\n",
    "    \n",
    "elif model_config.MODEL_NAME==\"EfficientNet\":\n",
    "    from efficientnet.keras import EfficientNetB5 as base_model\n",
    "    model_config.INPUT_SHAPE=(456,456,3)\n",
    "    \n",
    "model_config.NUM_CLASS=len(data_df[\"label\"].unique())\n",
    "\n",
    "date = datetime.now().strftime(\"%y%m%d\")\n",
    "if not hasattr(model_config, \"LOG_DIR\"):\n",
    "    model_config.LOG_DIR = os.path.join(model_config.SAVE_BASE_DIR,model_config.PROJ_NAME,\"logs_{}\".format(date),model_config.MODEL_NAME)\n",
    "\n",
    "    if not os.path.exists(model_config.LOG_DIR) :\n",
    "        os.makedirs(model_config.LOG_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. MODEL BUILD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1 MODEL INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL BUILD\n",
    "\n",
    "model=build_model(model_config=model_config, base_model=base_model)\n",
    "model_json = model.to_json()\n",
    "with open(os.path.join(model_config.LOG_DIR,'model.json'),\"w\") as f: f.write(model_json)\n",
    "\n",
    "# Load weights\n",
    "ckpt_list=sorted(glob(os.path.join(model_config.LOG_DIR, \"*.h5\")))\n",
    "\n",
    "if len(ckpt_list) > 0 :\n",
    "    find_best = [x for x in ckpt_list if \"train_best.h5\" in x]\n",
    "    if len(find_best) > 0:\n",
    "        ckpt_path=find_best[0]\n",
    "        init_epoch=0\n",
    "    else:\n",
    "        ckpt_path=ckpt_list[-1]\n",
    "        init_epoch=int(ckpt_path.split(\"-\")[-2])\n",
    "        \n",
    "    model.load_weights(ckpt_path)\n",
    "    print(\"Restart from {}\".format(ckpt_path))\n",
    "\n",
    "else :\n",
    "    [os.remove(x) for x in glob(os.path.join(model_config.LOG_DIR, \"*tfevents*\"))]\n",
    "    print(\"Start from initial weights !\")\n",
    "    init_epoch=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2 TRAINING INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Callbacks\n",
    "ckpt=ModelCheckpoint(os.path.join(model_config.LOG_DIR, 'epoch-{epoch:02d}-{val_acc:.5f}.h5'), \n",
    "                     monitor='val_acc', verbose=1, save_best_only=True)\n",
    "\n",
    "tboard=TensorBoard(log_dir=model_config.LOG_DIR, histogram_freq=0,\n",
    "                     write_graph=True, write_images=True)\n",
    "\n",
    "reducer1=ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=3, verbose=1,\n",
    "                          mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "\n",
    "reducer2=ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=5, verbose=1,\n",
    "                          mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "\n",
    "stopper=EarlyStopping(monitor='val_acc', min_delta=0, patience=10, verbose=1,\n",
    "                      mode='max', baseline=None, restore_best_weights=True)\n",
    "\n",
    "callbacks_list=[ckpt, tboard, stopper, reducer1, reducer2]\n",
    "\n",
    "# Make Generator\n",
    "train_generator=BalencedDataGenerator(data=train_df, config=model_config, is_train=True)\n",
    "valid_generator=BalencedDataGenerator(data=test_df, config=model_config, is_train=False)\n",
    "\n",
    "# Set Params\n",
    "train_count=train_df.shape[0]\n",
    "one_epoch=int(train_count/model_config.BATCH_SIZE)\n",
    "num_epochs=model_config.NUM_EPOCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. RUNNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-1 TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_config.MODE==\"train\":\n",
    "    \n",
    "    history=model.fit_generator(train_generator, \n",
    "                                steps_per_epoch=one_epoch,\n",
    "                                epochs=num_epochs,\n",
    "                                callbacks=callbacks_list,\n",
    "                                initial_epoch=init_epoch,\n",
    "                                verbose=1,\n",
    "                                validation_data=valid_generator,\n",
    "                                validation_steps=test_df.shape[0]//model_config.BATCH_SIZE,\n",
    "                                class_weight=class_weights)\n",
    "\n",
    "    model.save(os.path.join(model_config.LOG_DIR, 'train_best.h5'), include_optimizer = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_env",
   "language": "python",
   "name": "dl_tutorials"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
